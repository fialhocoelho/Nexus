{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run `TimeGPT-1` for Santos off-shore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.nexdata: Defining paths...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "import numpy as np\n",
    "from nixtla import NixtlaClient\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "import logging\n",
    "\n",
    "sys.path.append('../src/')\n",
    "from utils.nexdata import *\n",
    "from utils.nexutil import *\n",
    "\n",
    "# Simular argumentos da linha de comando\n",
    "sys.argv = ['timegpt.py', '-v']\n",
    "#sys.argv = ['timegpt.py']\n",
    "\n",
    "# Configure the root logger\n",
    "# Parse arguments\n",
    "args = parse_args()\n",
    "log_level = get_log_level(args.verbose)\n",
    "log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "\n",
    "logging.basicConfig(level=log_level, format=log_fmt)\n",
    "\n",
    "\n",
    "params = NexData(nexus_folder='../',\n",
    "                log_level = log_level)\n",
    "\n",
    "set_random_seeds(params.data_params['default_seed'])\n",
    "\n",
    "id_experiment = 'timegpt_forecast_target_features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring models, predict and save outputs to be used to `student` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.nexdata: TimeGPT model load with successfull o/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nixtla_client = NixtlaClient(\n",
    "        api_key = load_api_key(\"../config/nixtla_api.key\"),\n",
    "        max_retries=30,\n",
    "        retry_interval=30,\n",
    "    )\n",
    "    params.logger.info(f' TimeGPT model load with successfull o/')\n",
    "except Exception as err:\n",
    "    params.logger.critical(f' Chronos model cannot be loaded. {err}')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.nexdata: ################## Ocean variable: current_praticagem\n",
      "INFO:utils.nexdata: ################## Ocean variable: ssh_praticagem\n",
      "INFO:utils.nexdata: ################## Ocean variable: wind_praticagem\n",
      "INFO:utils.nexdata: ################## Ocean variable: waves_palmas\n",
      "INFO:utils.nexdata: ################## Ocean variable: sofs_praticagem\n",
      "INFO:utils.nexdata: ################## Ocean variable: astronomical_tide\n",
      "INFO:utils.nexdata: endg features:   ['datetime', 'current_praticagem_cross_shore_current', 'ssh_praticagem_ssh', 'wind_praticagem_vx', 'wind_praticagem_vy', 'waves_palmas_hs', 'waves_palmas_tp', 'waves_palmas_ws', 'sofs_praticagem_cross_shore_current', 'sofs_praticagem_ssh', 'astronomical_tide_astronomical_tide']\n",
      "INFO:utils.nexdata: exog features:   ['datetime', 'sofs_praticagem_cross_shore_current', 'sofs_praticagem_ssh', 'astronomical_tide_astronomical_tide']\n",
      "INFO:utils.nexdata: target features: ['current_praticagem_cross_shore_current', 'waves_palmas_hs', 'waves_palmas_tp', 'waves_palmas_ws']\n"
     ]
    }
   ],
   "source": [
    "df_train_composed = pd.DataFrame()\n",
    "\n",
    "train_range = pd.date_range(start=params.data_params['train_start_date'],\n",
    "                            end=params.data_params['train_end_date'],\n",
    "                            freq=params.data_params['target_freq'])\n",
    "\n",
    "df_train_composed[params.data_params['datetime_col']] = train_range\n",
    "\n",
    "df_test_composed = pd.DataFrame()\n",
    "\n",
    "endg_list = []\n",
    "exog_list = []\n",
    "target_feature_list = []\n",
    "\n",
    "endg_list.append(params.data_params['datetime_col'])\n",
    "exog_list.append(params.data_params['datetime_col'])\n",
    "\n",
    "test_range = pd.date_range(start=params.data_params['test_start_date'],\n",
    "                            end=params.data_params['test_end_date'],\n",
    "                            freq=params.data_params['target_freq'])\n",
    "\n",
    "df_test_composed[params.data_params['datetime_col']] = test_range\n",
    "\n",
    "# Iterate over each ocean variable defined in the parameters\n",
    "for ocean_variable in params.features.keys():\n",
    "    params.logger.info(f' ################## Ocean variable: {ocean_variable}')\n",
    "    # Retrieve target features and experiment IDs\n",
    "    target_features = params.features[ocean_variable]\n",
    "    params.logger.debug(f\" {target_features}\")\n",
    "    id_experiment = 'chronos_forecast_composed'\n",
    "    id_experiment_ioa = 'chronos_ioa_composed'\n",
    "\n",
    "    # Load train and test data for the target feature\n",
    "    df_train_target = pd.read_parquet(\n",
    "        target_features['train_filepath'])\n",
    "    df_test_target = pd.read_parquet(\n",
    "        target_features['test_filepath'])\n",
    "\n",
    "    try:\n",
    "        # Process the training dataframe with specified parameters\n",
    "        df_train_processed_target = process_dataframe(\n",
    "            df_train_target,\n",
    "            target_features['train_start_date'],\n",
    "            target_features['train_end_date'],\n",
    "            params.data_params['target_freq'],\n",
    "            params.data_params['interp_method'],\n",
    "            params.data_params['datetime_col'],\n",
    "            target_features['freq'])\n",
    "            #\"20min\")\n",
    "        #params.logger.debug(' df_train_target are processed.')\n",
    "        params.logger.debug(f' Train cols: {df_train_processed_target.columns}')\n",
    "\n",
    "        # Process the test dataframe with specified parameters\n",
    "        df_test_processed_target = process_dataframe(\n",
    "            df_test_target,\n",
    "            target_features['test_start_date'],\n",
    "            target_features['test_end_date'],\n",
    "            params.data_params['target_freq'],\n",
    "            params.data_params['interp_method'],\n",
    "            params.data_params['datetime_col'],\n",
    "            target_features['freq'])\n",
    "            #\"20min\")\n",
    "        #params.logger.debug(' df_test_target are processed.')\n",
    "        params.logger.debug(f' Test cols: {df_test_processed_target.columns}')\n",
    "    except Exception as e:\n",
    "        params.logger.debug(f\" Error {e} on {ocean_variable}\")\n",
    "\n",
    "    # Define the context and forecast window lengths and shift\n",
    "    context_len = params.model_params['context_window_len']\n",
    "    forecast_len = params.model_params['forecast_len']\n",
    "    shift = params.model_params['shift']\n",
    "    mode = params.model_params['windowing_mode']\n",
    "\n",
    "    # Generate indices for the test set using the context and forecast lengths\n",
    "    X_test_index, y_test_index = generate_indices(\n",
    "        df_test_processed_target, context_len, forecast_len,\n",
    "        shift, mode)\n",
    "    #params.logger.debug(' X_test_index, y_test_index are created.')\n",
    "\n",
    "    # Initialize DataFrames for predictions and index of agreement (IOA) values\n",
    "    df_y_hat = pd.DataFrame()\n",
    "    df_ioa = pd.DataFrame()\n",
    "\n",
    "    # Set the index for the y_hat DataFrame\n",
    "    df_y_hat.index = np.concatenate(y_test_index)\n",
    "    df_y_hat[params.data_params['datetime_col']] = (\n",
    "        df_test_processed_target.loc[\n",
    "            df_y_hat.index, params.data_params['datetime_col']\n",
    "        ])\n",
    "\n",
    "    #params.logger.debug(' start loop from df features')\n",
    "    # Iterate over each target feature for prediction\n",
    "    for target_feature in target_features['list_features']:\n",
    "        # Add training data to improve the size of the inference data\n",
    "        train_signal = df_train_processed_target.loc[:, \n",
    "            target_feature]\n",
    "\n",
    "        test_signal = df_test_processed_target.loc[:, \n",
    "            target_feature]\n",
    "        \n",
    "        len_X_test_index = len(X_test_index)\n",
    "\n",
    "        df_target_col_name = f'{ocean_variable}_{target_feature}'\n",
    "        params.logger.debug(\n",
    "            f\" Target feature: {target_feature} | {df_target_col_name}\")\n",
    "        \n",
    "\n",
    "        df_train_composed[df_target_col_name] = train_signal.values\n",
    "        df_test_composed[df_target_col_name] = test_signal.values\n",
    "\n",
    "        endg_list.append(df_target_col_name)\n",
    "        if target_features['exog']: exog_list.append(df_target_col_name)\n",
    "        if target_features['target_feature']: target_feature_list.append(\n",
    "            df_target_col_name)\n",
    "\n",
    "params.logger.info(f' endg features:   {endg_list}')\n",
    "params.logger.info(f' exog features:   {exog_list}')\n",
    "params.logger.info(f' target features: {target_feature_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [33:55<00:00, 11.37s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>current_praticagem_cross_shore_current</th>\n",
       "      <th>waves_palmas_hs</th>\n",
       "      <th>waves_palmas_tp</th>\n",
       "      <th>waves_palmas_ws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2022-01-08 00:00:00</td>\n",
       "      <td>-0.496241</td>\n",
       "      <td>1.205139</td>\n",
       "      <td>10.518562</td>\n",
       "      <td>-0.015064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>2022-01-08 01:00:00</td>\n",
       "      <td>-0.475252</td>\n",
       "      <td>1.117056</td>\n",
       "      <td>10.122596</td>\n",
       "      <td>0.002176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2022-01-08 02:00:00</td>\n",
       "      <td>-0.221233</td>\n",
       "      <td>1.495980</td>\n",
       "      <td>10.170714</td>\n",
       "      <td>0.200205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2022-01-08 03:00:00</td>\n",
       "      <td>-0.304862</td>\n",
       "      <td>1.322694</td>\n",
       "      <td>9.801672</td>\n",
       "      <td>0.191701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2022-01-08 04:00:00</td>\n",
       "      <td>-0.258456</td>\n",
       "      <td>1.231174</td>\n",
       "      <td>10.200695</td>\n",
       "      <td>0.186209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>2022-12-31 19:00:00</td>\n",
       "      <td>0.117516</td>\n",
       "      <td>0.660472</td>\n",
       "      <td>9.674246</td>\n",
       "      <td>0.078386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>2022-12-31 20:00:00</td>\n",
       "      <td>0.144268</td>\n",
       "      <td>0.662330</td>\n",
       "      <td>9.598036</td>\n",
       "      <td>0.114077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>2022-12-31 21:00:00</td>\n",
       "      <td>-0.021904</td>\n",
       "      <td>0.564333</td>\n",
       "      <td>9.813744</td>\n",
       "      <td>0.033776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>2022-12-31 22:00:00</td>\n",
       "      <td>0.150519</td>\n",
       "      <td>0.658883</td>\n",
       "      <td>9.335948</td>\n",
       "      <td>0.162465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>0.177804</td>\n",
       "      <td>0.735480</td>\n",
       "      <td>9.691661</td>\n",
       "      <td>0.144297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8592 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                datetime  current_praticagem_cross_shore_current  \\\n",
       "168  2022-01-08 00:00:00                               -0.496241   \n",
       "169  2022-01-08 01:00:00                               -0.475252   \n",
       "170  2022-01-08 02:00:00                               -0.221233   \n",
       "171  2022-01-08 03:00:00                               -0.304862   \n",
       "172  2022-01-08 04:00:00                               -0.258456   \n",
       "...                  ...                                     ...   \n",
       "8755 2022-12-31 19:00:00                                0.117516   \n",
       "8756 2022-12-31 20:00:00                                0.144268   \n",
       "8757 2022-12-31 21:00:00                               -0.021904   \n",
       "8758 2022-12-31 22:00:00                                0.150519   \n",
       "8759 2022-12-31 23:00:00                                0.177804   \n",
       "\n",
       "      waves_palmas_hs  waves_palmas_tp  waves_palmas_ws  \n",
       "168          1.205139        10.518562        -0.015064  \n",
       "169          1.117056        10.122596         0.002176  \n",
       "170          1.495980        10.170714         0.200205  \n",
       "171          1.322694         9.801672         0.191701  \n",
       "172          1.231174        10.200695         0.186209  \n",
       "...               ...              ...              ...  \n",
       "8755         0.660472         9.674246         0.078386  \n",
       "8756         0.662330         9.598036         0.114077  \n",
       "8757         0.564333         9.813744         0.033776  \n",
       "8758         0.658883         9.335948         0.162465  \n",
       "8759         0.735480         9.691661         0.144297  \n",
       "\n",
       "[8592 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fcst_df_full = pd.DataFrame()\n",
    "\n",
    "fcst_df_full.index = pd.RangeIndex(start=context_len,\n",
    "                                   stop=df_test_composed.shape[0],\n",
    "                                   step=1)\n",
    "fcst_df_full[params.data_params['datetime_col']] = df_y_hat[\n",
    "    params.data_params['datetime_col']]\n",
    "\n",
    "for tgt_feature in target_feature_list:\n",
    "    params.logger.info(f' target feature to forecast: {tgt_feature}')\n",
    "    dt_list = []\n",
    "    fcst_list = []\n",
    "    for idx in tqdm(range(len_X_test_index)):\n",
    "        params.logger.info(f' \\ntarget feature: {tgt_feature}')\n",
    "        # Extract test signal for the current window\n",
    "        X_test_df = df_test_composed.loc[X_test_index[idx], :]\n",
    "        y_test_df = df_test_composed.loc[y_test_index[idx], :]\n",
    "\n",
    "        # Concatenate training and test signals\n",
    "        combined_df = pd.concat([df_train_composed,\n",
    "                                    X_test_df], axis=0).reset_index(drop=True)\n",
    "        \n",
    "        attempts = params.model_params['attempts_after_failure']\n",
    "\n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                fcst_df = nixtla_client.forecast(\n",
    "                    df=combined_df,\n",
    "                    h=params.model_params['forecast_len'],\n",
    "                    freq=params.data_params['target_freq'],\n",
    "                    time_col=params.data_params['datetime_col'],\n",
    "                    target_col=tgt_feature,\n",
    "                    X_df=y_test_df[exog_list],  # Somente features exógenas\n",
    "                    model=params.model_params['timegpt_model'],\n",
    "                    #finetune_steps=params.model_params[\n",
    "                    # 'timegpt_finetune_steps'],\n",
    "                )\n",
    "                break  # Sai do loop se a previsão for bem-sucedida\n",
    "            except Exception as e:\n",
    "                params.logger.info(\n",
    "                    f'Error at attempt {attempt + 1} for {tgt_feature}: {e}')\n",
    "                if attempt <= attempts:\n",
    "                    time.sleep(10)  # Aguarda 10 segundos antes de tentar dnv\n",
    "                else:\n",
    "                    params.logger.error(\n",
    "                        f'Fail after {attempts} attempts'\n",
    "                    )\n",
    "                    sys.exit(1)\n",
    "                \n",
    "        dt_list.extend(fcst_df.datetime.values)\n",
    "        fcst_list.extend(fcst_df.TimeGPT.values)\n",
    "        clear_output(wait=True)\n",
    "        #TODO: Pegar tbm o valor medido para entrar no df\n",
    "\n",
    "    fcst_df_full[tgt_feature] = fcst_list\n",
    "\n",
    "# Save the predictions DataFrame to a parquet file\n",
    "filename = os.path.join(\n",
    "    params.forecasted_dir,\n",
    "    f\"{id_experiment}_\"\n",
    "    f\"{params.timestamp}.pkl\")\n",
    "\n",
    "params.logger.info(f'Output file: {filename}')\n",
    "\n",
    "fcst_df_full.to_parquet(filename)\n",
    "\n",
    "display(fcst_df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tgt_feature in target_feature_list:\n",
    "    for idx in range(150, 160):\n",
    "        print(f'ioa: {calculate_ioa(df_test_composed.loc[y_test_index[idx],\n",
    "                                                        tgt_feature].values,\n",
    "                                        fcst_df_full.loc[y_test_index[idx],\n",
    "                                                        tgt_feature].values)}')\n",
    "        plt.plot(fcst_df_full.loc[y_test_index[idx],'datetime'],\n",
    "            fcst_df_full.loc[y_test_index[idx],tgt_feature],\n",
    "            label=tgt_feature)\n",
    "        plt.plot(df_test_composed.loc[y_test_index[idx],'datetime'],\n",
    "            df_test_composed.loc[y_test_index[idx],tgt_feature],\n",
    "            label='Measured')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
