{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run `TimeGPT-1` for Santos off-shore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.nexdata: Defining paths...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "import numpy as np\n",
    "from nixtla import NixtlaClient\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "import logging\n",
    "\n",
    "sys.path.append('../src/')\n",
    "from utils.nexdata import *\n",
    "from utils.nexutil import *\n",
    "\n",
    "# Simular argumentos da linha de comando\n",
    "sys.argv = ['timegpt.py', '-v']\n",
    "#sys.argv = ['timegpt.py']\n",
    "\n",
    "# Configure the root logger\n",
    "# Parse arguments\n",
    "args = parse_args()\n",
    "log_level = get_log_level(args.verbose)\n",
    "log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "\n",
    "logging.basicConfig(level=log_level, format=log_fmt)\n",
    "\n",
    "\n",
    "params = NexData(nexus_folder='../',\n",
    "                log_level = log_level)\n",
    "\n",
    "set_random_seeds(params.data_params['default_seed'])\n",
    "\n",
    "id_experiment = 'timegpt_znorm_forecast_target_features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring models, predict and save outputs to be used to `student` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.nexdata: TimeGPT model load with successfull o/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nixtla_client = NixtlaClient(\n",
    "        api_key = load_api_key(\"../config/nixtla_api.key\"),\n",
    "        max_retries=params.model_params['attempts_after_failure'],\n",
    "        retry_interval=params.model_params['retry_interval'],\n",
    "    )\n",
    "    params.logger.info(f' TimeGPT model load with successfull o/')\n",
    "except Exception as err:\n",
    "    params.logger.critical(f' Chronos model cannot be loaded. {err}')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.nexdata: ################## Ocean variable: current_praticagem\n",
      "INFO:utils.nexdata: ################## Ocean variable: ssh_praticagem\n",
      "INFO:utils.nexdata: ################## Ocean variable: wind_praticagem\n",
      "INFO:utils.nexdata: ################## Ocean variable: waves_palmas\n",
      "INFO:utils.nexdata: ################## Ocean variable: sofs_praticagem\n",
      "INFO:utils.nexdata: ################## Ocean variable: astronomical_tide\n",
      "INFO:utils.nexdata: endg features:   ['datetime', 'current_praticagem_cross_shore_current', 'ssh_praticagem_ssh', 'wind_praticagem_vx', 'wind_praticagem_vy', 'waves_palmas_hs', 'waves_palmas_tp', 'waves_palmas_ws', 'sofs_praticagem_cross_shore_current', 'sofs_praticagem_ssh', 'astronomical_tide_astronomical_tide']\n",
      "INFO:utils.nexdata: exog features:   ['datetime', 'sofs_praticagem_cross_shore_current', 'sofs_praticagem_ssh', 'astronomical_tide_astronomical_tide']\n",
      "INFO:utils.nexdata: target features: ['current_praticagem_cross_shore_current', 'waves_palmas_hs', 'waves_palmas_tp', 'waves_palmas_ws']\n"
     ]
    }
   ],
   "source": [
    "df_train_composed = pd.DataFrame()\n",
    "\n",
    "train_range = pd.date_range(start=params.data_params['train_start_date'],\n",
    "                            end=params.data_params['train_end_date'],\n",
    "                            freq=params.data_params['target_freq'])\n",
    "\n",
    "df_train_composed[params.data_params['datetime_col']] = train_range\n",
    "\n",
    "df_test_composed = pd.DataFrame()\n",
    "\n",
    "endg_list = []\n",
    "exog_list = []\n",
    "target_feature_list = []\n",
    "\n",
    "endg_list.append(params.data_params['datetime_col'])\n",
    "exog_list.append(params.data_params['datetime_col'])\n",
    "\n",
    "test_range = pd.date_range(start=params.data_params['test_start_date'],\n",
    "                            end=params.data_params['test_end_date'],\n",
    "                            freq=params.data_params['target_freq'])\n",
    "\n",
    "df_test_composed[params.data_params['datetime_col']] = test_range\n",
    "\n",
    "# Iterate over each ocean variable defined in the parameters\n",
    "for ocean_variable in params.features.keys():\n",
    "    params.logger.info(f' ################## Ocean variable: {ocean_variable}')\n",
    "    # Retrieve target features and experiment IDs\n",
    "    target_features = params.features[ocean_variable]\n",
    "    params.logger.debug(f\" {target_features}\")\n",
    "    id_experiment = 'chronos_forecast_composed'\n",
    "    id_experiment_ioa = 'chronos_ioa_composed'\n",
    "\n",
    "    # Load train and test data for the target feature\n",
    "    df_train_target = pd.read_parquet(\n",
    "        target_features['train_filepath'])\n",
    "    df_test_target = pd.read_parquet(\n",
    "        target_features['test_filepath'])\n",
    "    #print(f'Train filepath: {target_features['train_filepath']}')\n",
    "    #print(f'Test filepath: {target_features['test_filepath']}')\n",
    "\n",
    "    try:\n",
    "        # Process the training dataframe with specified parameters\n",
    "        df_train_processed_target = process_dataframe(\n",
    "            df_train_target,\n",
    "            target_features['train_start_date'],\n",
    "            target_features['train_end_date'],\n",
    "            params.data_params['target_freq'],\n",
    "            params.data_params['interp_method'],\n",
    "            params.data_params['datetime_col'],\n",
    "            target_features['freq'])\n",
    "            #\"20min\")\n",
    "        #params.logger.debug(' df_train_target are processed.')\n",
    "        params.logger.debug(f' Train cols: {df_train_processed_target.columns}')\n",
    "\n",
    "        # Process the test dataframe with specified parameters\n",
    "        df_test_processed_target = process_dataframe(\n",
    "            df_test_target,\n",
    "            target_features['test_start_date'],\n",
    "            target_features['test_end_date'],\n",
    "            params.data_params['target_freq'],\n",
    "            params.data_params['interp_method'],\n",
    "            params.data_params['datetime_col'],\n",
    "            target_features['freq'])\n",
    "            #\"20min\")\n",
    "        #params.logger.debug(' df_test_target are processed.')\n",
    "        params.logger.debug(f' Test cols: {df_test_processed_target.columns}')\n",
    "    except Exception as e:\n",
    "        params.logger.debug(f\" Error {e} on {ocean_variable}\")\n",
    "\n",
    "    # Define the context and forecast window lengths and shift\n",
    "    context_len = params.model_params['context_window_len']\n",
    "    forecast_len = params.model_params['forecast_len']\n",
    "    shift = params.model_params['shift']\n",
    "    mode = params.model_params['windowing_mode']\n",
    "\n",
    "    # Generate indices for the test set using the context and forecast lengths\n",
    "    X_test_index, y_test_index = generate_indices(\n",
    "        df_test_processed_target, context_len, forecast_len,\n",
    "        shift, mode)\n",
    "\n",
    "    len_X_test_index = len(X_test_index)\n",
    "\n",
    "    # Initialize DataFrames for predictions and index of agreement (IOA) values\n",
    "    df_y_hat = pd.DataFrame()\n",
    "    df_ioa = pd.DataFrame()\n",
    "\n",
    "    # Set the index for the y_hat DataFrame\n",
    "    df_y_hat.index = np.concatenate(y_test_index)\n",
    "    df_y_hat[params.data_params['datetime_col']] = (\n",
    "        df_test_processed_target.loc[\n",
    "            df_y_hat.index, params.data_params['datetime_col']\n",
    "        ])\n",
    "\n",
    "    # Iterate over each target feature for prediction\n",
    "    for target_feature in target_features['list_features']:\n",
    "        # Add training data to improve the size of the inference data\n",
    "        train_signal = df_train_processed_target.loc[:, \n",
    "            target_feature]\n",
    "\n",
    "        test_signal = df_test_processed_target.loc[:, \n",
    "            target_feature]\n",
    "\n",
    "        df_target_col_name = f'{ocean_variable}_{target_feature}'\n",
    "        params.logger.debug(\n",
    "            f\" Target feature: {target_feature} | {df_target_col_name}\")\n",
    "\n",
    "        df_train_composed[df_target_col_name] = train_signal.values\n",
    "        df_test_composed[df_target_col_name] = test_signal.values\n",
    "\n",
    "        endg_list.append(df_target_col_name)\n",
    "        if target_features['exog']: exog_list.append(df_target_col_name)\n",
    "        if target_features['target_feature']: target_feature_list.append(\n",
    "            df_target_col_name)\n",
    "\n",
    "params.logger.info(f' endg features:   {endg_list}')\n",
    "params.logger.info(f' exog features:   {exog_list}')\n",
    "params.logger.info(f' target features: {target_feature_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 33/179 [04:41<20:46,  8.54s/it]INFO:utils.nexdata: \n",
      "target feature: current_praticagem_cross_shore_current\n",
      "INFO:nixtla.nixtla_client:Validating inputs...\n",
      "INFO:nixtla.nixtla_client:Preprocessing dataframes...\n",
      "INFO:nixtla.nixtla_client:Using the following exogenous variables: sofs_praticagem_cross_shore_current, sofs_praticagem_ssh, astronomical_tide_astronomical_tide\n",
      "INFO:nixtla.nixtla_client:Calling Forecast Endpoint...\n"
     ]
    }
   ],
   "source": [
    "fcst_df_full = pd.DataFrame()\n",
    "\n",
    "fcst_df_full.index = pd.RangeIndex(start=context_len,\n",
    "                                    stop=df_test_composed.shape[0],\n",
    "                                    step=1)\n",
    "fcst_df_full[params.data_params['datetime_col']] = df_y_hat[\n",
    "    params.data_params['datetime_col']]\n",
    "\n",
    "for tgt_feature in target_feature_list:\n",
    "    params.logger.info(f' target feature to forecast: {tgt_feature}')\n",
    "    dt_list = []\n",
    "    fcst_list = []\n",
    "    for idx in tqdm(range(len_X_test_index)):\n",
    "        params.logger.info(f' \\ntarget feature: {tgt_feature}')\n",
    "        # Extract test signal for the current window\n",
    "        X_test_df = df_test_composed.loc[X_test_index[idx], :]\n",
    "        y_test_df = df_test_composed.loc[y_test_index[idx], :]\n",
    "\n",
    "        # Concatenate training and test signals\n",
    "        if params.model_params['normalize']:\n",
    "            aux_combined_df = pd.concat([df_train_composed,\n",
    "                                        X_test_df], axis=0).reset_index(\n",
    "                                            drop=True)\n",
    "            combined_df, _,_ = normalize_z_score_df(aux_combined_df)\n",
    "            df_cct = pd.concat([df_train_composed,y_test_df], axis=0)\n",
    "            _, means, stds = normalize_z_score_df(df_cct.reset_index(drop=True))\n",
    "            y_test_df_norm , _, _ = normalize_z_score_df(y_test_df, means, stds)\n",
    "        else:\n",
    "            combined_df = pd.concat([df_train_composed,\n",
    "                                    X_test_df], axis=0).reset_index(drop=True)           \n",
    "        \n",
    "        attempts = params.model_params['attempts_after_failure']\n",
    "\n",
    "        exog_df = y_test_df_norm[exog_list] if params.model_params['normalize']\\\n",
    "            else y_test_df[exog_list] # Somente features exógenas\n",
    "\n",
    "        for attempt in range(attempts):\n",
    "            try:\n",
    "                fcst_df = nixtla_client.forecast(\n",
    "                    df=combined_df,\n",
    "                    h=params.model_params['forecast_len'],\n",
    "                    freq=params.data_params['target_freq'],\n",
    "                    time_col=params.data_params['datetime_col'],\n",
    "                    target_col=tgt_feature,\n",
    "                    X_df = exog_df,\n",
    "                    model=params.model_params['timegpt_model'],\n",
    "                    #finetune_steps=params.model_params[\n",
    "                    # 'timegpt_finetune_steps'],\n",
    "                )\n",
    "                break  # Sai do loop se a previsão for bem-sucedida\n",
    "            except Exception as e:\n",
    "                params.logger.info(\n",
    "                    f'Error at attempt {attempt + 1} for {tgt_feature}: {e}')\n",
    "                if attempt <= attempts:\n",
    "                    time.sleep(10)  # Aguarda 10 segundos antes de tentar dnv\n",
    "                else:\n",
    "                    params.logger.error(\n",
    "                        f'Fail after {attempts} attempts'\n",
    "                    )\n",
    "                    sys.exit(1)\n",
    "                \n",
    "        dt_list.extend(fcst_df.datetime.values)\n",
    "        if params.model_params['normalize']:\n",
    "            fcst_list.extend(denormalize_z_score(fcst_df.TimeGPT.values,\n",
    "                                                means[tgt_feature],\n",
    "                                                stds[tgt_feature]))\n",
    "        else:\n",
    "            fcst_list.extend(fcst_df.TimeGPT.values)\n",
    "        clear_output(wait=True)\n",
    "        #TODO: Pegar tbm o valor medido para entrar no df\n",
    "\n",
    "    fcst_df_full[tgt_feature] = fcst_list\n",
    "\n",
    "# Save the predictions DataFrame to a parquet file\n",
    "filename = os.path.join(\n",
    "    params.forecasted_dir,\n",
    "    f\"{id_experiment}_\"\n",
    "    f\"{params.timestamp}.pkl\")\n",
    "\n",
    "params.logger.info(f' Output file: {filename}')\n",
    "\n",
    "fcst_df_full.to_parquet(filename)\n",
    "\n",
    "display(fcst_df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tgt_feature in ['waves_palmas_hs']:\n",
    "    ioa_list_norm = []\n",
    "    ioa_list_non_norm = []\n",
    "    for idx in range(len(y_test_index)):\n",
    "        ioa_norm = calculate_ioa(df_test_composed.loc[y_test_index[idx],tgt_feature].values,\n",
    "                                    fcst_df_full.loc[y_test_index[idx],tgt_feature].values)\n",
    "        ioa_list_norm.append(ioa_norm)\n",
    "\n",
    "        ioa_non_norm = calculate_ioa(df_test_composed.loc[y_test_index[idx], tgt_feature].values,\n",
    "                                    fcst_df_without_normalized.loc[y_test_index[idx],tgt_feature].values)\n",
    "        ioa_list_non_norm.append(ioa_non_norm)\n",
    "\n",
    "    plt.plot(np.cumsum(ioa_list_norm), label='norm')\n",
    "    plt.plot(np.cumsum(ioa_list_non_norm), label='non_norm')\n",
    "    #plt.plot(df_test_composed.loc[y_test_index[idx],'datetime'],\n",
    "    #    df_test_composed.loc[y_test_index[idx],tgt_feature],\n",
    "    #    label='Measured')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'{tgt_feature} norm mean:     {np.std(ioa_list_norm)}')\n",
    "    print(f'{tgt_feature} non-norm mean: {np.std(ioa_list_non_norm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tgt_feature in ['waves_palmas_hs']:\n",
    "    for idx in range(150, 160):\n",
    "        print(f'ioa norm:     {calculate_ioa(df_test_composed.loc[y_test_index[idx],\n",
    "                                                        tgt_feature].values,\n",
    "                                        fcst_df_full.loc[y_test_index[idx],\n",
    "                                                        tgt_feature].values)}')\n",
    "        print(f'ioa non_norm: {calculate_ioa(df_test_composed.loc[y_test_index[idx],\n",
    "                                                        tgt_feature].values,\n",
    "                                        fcst_df_without_normalized.loc[y_test_index[idx],\n",
    "                                                        tgt_feature].values)}')\n",
    "        plt.plot(fcst_df_full.loc[y_test_index[idx],'datetime'],\n",
    "            fcst_df_full.loc[y_test_index[idx],tgt_feature],\n",
    "            label=tgt_feature)\n",
    "        plt.plot(fcst_df_without_normalized.loc[y_test_index[idx],'datetime'],\n",
    "            fcst_df_without_normalized.loc[y_test_index[idx],tgt_feature],\n",
    "            label='non_norm')\n",
    "        plt.plot(df_test_composed.loc[y_test_index[idx],'datetime'],\n",
    "            df_test_composed.loc[y_test_index[idx],tgt_feature],\n",
    "            label='Measured')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tgt_feature in target_feature_list:\n",
    "    for idx in range(150, 160):\n",
    "        print(f'ioa: {calculate_ioa(df_test_composed.loc[y_test_index[idx],\n",
    "                                                        tgt_feature].values,\n",
    "                                        fcst_df_full.loc[y_test_index[idx],\n",
    "                                                        tgt_feature].values)}')\n",
    "        plt.plot(fcst_df_full.loc[y_test_index[idx],'datetime'],\n",
    "            fcst_df_full.loc[y_test_index[idx],tgt_feature],\n",
    "            label=tgt_feature)\n",
    "        plt.plot(df_test_composed.loc[y_test_index[idx],'datetime'],\n",
    "            df_test_composed.loc[y_test_index[idx],tgt_feature],\n",
    "            label='Measured')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
